<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>post on Samden Lepcha</title>
    <link>/blogs/</link>
    <description>Recent content in post on Samden Lepcha</description>
    <generator>Source Themes academia (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; {year} Samden Lepcha</copyright>
    <lastBuildDate>Mon, 13 Jul 2020 00:00:00 +0000</lastBuildDate>
    
	    <atom:link href="/blogs/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Pothole Detection with Mask RCNN</title>
      <link>/blogs/pothole-detection-mask-rcnn/</link>
      <pubDate>Mon, 13 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/blogs/pothole-detection-mask-rcnn/</guid>
      <description>&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;According to Wikipedia &amp;ldquo;A pothole is a depression in a road surface, usually asphalt pavement, where traffic has removed broken pieces of the pavement&amp;rdquo;. Edmonton the &amp;ldquo;self proclaimed pothole capital&amp;rdquo; in Alberta, Canada reportedly spends $4.8 million on 450,000 potholes annually, as of 2015. In India every year approximately 1100 lives are lost to accidents caused by potholes &lt;a href=&#34;https://www.indiatoday.in/india/story/over-9300-deaths-25000-injured-in-3-years-due-to-potholes-1294147-2018-07-24&#34;&gt;source&lt;/a&gt;. Ordinary citizens do not have the means of communicating or reporting the bad roads to the concerned authorities while the authorities lay unaware of the situation.&lt;/p&gt;
&lt;p&gt;Therefore, several organisations have been trying to develop tools (like webapps) where the citizens can report the potholes to the concerned authorities. There are several hackathons that have taken place with this project in mind as one of the objectives. Seeing this as a growing concern, in this project to address this problem the aim is to &lt;strong&gt;Develop a simple interface that uses the state of the art object detection technology to detect potholes in real time and report them using Google Maps&lt;/strong&gt;. This article will take you through the steps required to build your very own pothole detection system. The deployment medium for this project will be on smartphones which are used by 500 million+ people in India according to Newzoo&amp;rsquo;s 2019 Global Mobile Market Report.&lt;/p&gt;
&lt;p&gt;Tools Used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Python 3.6+&lt;/li&gt;
&lt;li&gt;Tensorflow Object Detection API&lt;/li&gt;
&lt;li&gt;Pixel Annotation Tool&lt;/li&gt;
&lt;li&gt;Anaconda Package Manager&lt;/li&gt;
&lt;li&gt;Flask&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The workflow of the Project will be as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Environment Setup&lt;/li&gt;
&lt;li&gt;Dataset Gathering&lt;/li&gt;
&lt;li&gt;Model Training&lt;/li&gt;
&lt;li&gt;Deployment with Flask&lt;/li&gt;
&lt;li&gt;Results&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;anaconda-environment-setup&#34;&gt;Anaconda Environment Setup&lt;/h3&gt;
&lt;p&gt;In the Beginning we will setup a new Anaconda environment and install all the necessary packages required for this project. Anaconda is a popular python package manager alongside &amp;ldquo;pip&amp;rdquo;. If you have not installed prior to this project please install it using this link.&lt;/p&gt;
&lt;p&gt;It is a fairly straight forward installation and should not take long. You can install the &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt; if you have some experience using the command line but if you want the GUI you can install the &lt;a href=&#34;https://www.anaconda.com/products/individual&#34;&gt;Anaconda Navigator&lt;/a&gt; with all the additional packages (this will take longer to install).&lt;/p&gt;
&lt;p&gt;After this open &amp;ldquo;Anaconda Prompt&amp;rdquo; from your start menu and follow the rest of the installation instructions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create the conda environment.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(base) C:\Users&amp;gt;conda create --name pothole python=3.6
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Activate the environment and upgrade pip.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(base) C:\Users&amp;gt;activate pothole
(pothole) C:\Users&amp;gt;python -m pip install --upgrade pip
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Install the other necessary packages by issuing the following commands:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(pothole) C:\Users&amp;gt;conda install -c anaconda protobuf
(pothole) C:\Users&amp;gt;pip install pillow
(pothole) C:\Users&amp;gt;pip install lxml
(pothole) C:\Users&amp;gt;pip install Cython
(pothole) C:\Users&amp;gt;pip install contextlib2
(pothole) C:\Users&amp;gt;pip install jupyter
(pothole) C:\Users&amp;gt;pip install matplotlib
(pothole) C:\Users&amp;gt;pip install opencv-python
(pothole) C:\Users&amp;gt;pip install labelme
(pothole) C:\Users&amp;gt;pip install tensorflow-gpu==1.13.1
(pothole) C:\Users&amp;gt;pip install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Clone or download the tensorflow object detection api repostiory from Github. For the purpose of this project we are using tensorflow version 1.13.1. &lt;strong&gt;Note Always make sure the tensorflow version installed and the tensorflow object detection api repository version is the same&lt;/strong&gt;. Run the following command or download this &lt;a href=&#34;https://github.com/tensorflow/models/tree/r1.13.0&#34;&gt;repository&lt;/a&gt; manually.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(pothole) C:\Users&amp;gt;git clone https://github.com/tensorflow/models/tree/r1.13.0.git
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Place these folders in a folder called &amp;ldquo;models&amp;rdquo;. You can place this &amp;ldquo;models&amp;rdquo; folder in a directory of your choice.&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Configure the PYTHONPATH environment variable:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(pothole) C:\Users&amp;gt;set PYTHONPATH=C:\models;C:\models\research;C:\models\research\slim
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Change the directory location based on where you have store the models folder.&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Compile Protobufs and run setup.py&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the Ananconda Prompt change directories to \models\research directory:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(pothole) C:\Users&amp;gt;cd C:\models\research
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;Run the following lines of code:&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;protoc --python_out=. .\object_detection\protos\anchor_generator.proto .\object_detection\protos\argmax_matcher.proto .\object_detection\protos\bipartite_matcher.proto .\object_detection\protos\box_coder.proto .\object_detection\protos\box_predictor.proto .\object_detection\protos\eval.proto .\object_detection\protos\faster_rcnn.proto .\object_detection\protos\faster_rcnn_box_coder.proto .\object_detection\protos\grid_anchor_generator.proto .\object_detection\protos\hyperparams.proto .\object_detection\protos\image_resizer.proto .\object_detection\protos\input_reader.proto .\object_detection\protos\losses.proto .\object_detection\protos\matcher.proto .\object_detection\protos\mean_stddev_box_coder.proto .\object_detection\protos\model.proto .\object_detection\protos\optimizer.proto .\object_detection\protos\pipeline.proto .\object_detection\protos\post_processing.proto .\object_detection\protos\preprocessor.proto .\object_detection\protos\region_similarity_calculator.proto .\object_detection\protos\square_box_coder.proto .\object_detection\protos\ssd.proto .\object_detection\protos\ssd_anchor_generator.proto .\object_detection\protos\string_int_label_map.proto .\object_detection\protos\train.proto .\object_detection\protos\keypoint_box_coder.proto .\object_detection\protos\multiscale_anchor_generator.proto .\object_detection\protos\graph_rewriter.proto .\object_detection\protos\calibration.proto .\object_detection\protos\flexible_grid_anchor_generator.proto
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If it gives an error that the protobuf file could not be found run this after:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;protoc object_detection/protos/*.proto --python_out=.
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Finally we need to run the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(pothole) C:\models\research&amp;gt; python setup.py build
(pothole) C:\models\research&amp;gt; python setup.py install
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;You can test if everything is working out by running the IPython Notebook present in the object_detection folder called &amp;ldquo;object_detection_tutorial.ipynb&amp;rdquo;.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(pothole) C:\models\research&amp;gt;cd object_detection
(pothole) C:\models\research\object_detection&amp;gt;jupyter notebook object_detection_tutorial.ipynb
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;dataset-gathering&#34;&gt;Dataset Gathering&lt;/h3&gt;
&lt;p&gt;As always in the beginning of any Data Science or AI Project after the problem statement has been identified we move on to gathering the data or in this case images for training.&lt;/p&gt;
&lt;p&gt;To train a robust model we need to use a lot of pictures but with variation as well. That means the potholes must be present at various angles or shapes so that our model does not lean on to one variation or in other words overfits and does not generalise for other images.&lt;/p&gt;
&lt;p&gt;You can use the images that you have taken personally or download them from the Internet like me. For this project the idea is to detect potholes so we would not be segmenting them out based on severity but that does leave something for future scope as well. The following data sources were used for building this project:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.kaggle.com/atulyakumar98/pothole-detection-dataset?select=potholes&#34;&gt;Kaggle&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.researchgate.net/publication/282807920_Dataset_of_images_used_for_pothole_detection&#34;&gt;Research Gate&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the Images used in this dataset looks this:
&lt;img src=&#34;gallery/Dataset.jpg&#34; alt=&#34;Some of the Images in the dataset&#34;&gt;&lt;/p&gt;
&lt;p&gt;We need to resize the images so that the model can be train using these resized images like 800 x 600 in this project (Unless you have unlimited GPU compute power). Use either the command prompt or anaconda prompt or any other IDE to run this script. For example in Anaconda Prompt:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(base) C:\Users&amp;gt; python DatasetCreation.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The above mentioned script is available is in my Github Repo. This script will resize all the images in your directory and also perform random Train/Test Split with 80/20 splitting criteria. Now place these images inside models/research/object_detection/images to have all the data for this project in one place.&lt;/p&gt;
&lt;h3 id=&#34;data-labeling&#34;&gt;Data Labeling&lt;/h3&gt;
&lt;p&gt;Now that we have gathered the dataset we need to label the images so that the model understands what is a pothole. To label the images we need a labeling software.&lt;/p&gt;
&lt;p&gt;For the purpose of project, I have used labelme as it is fairly simple to use. In your anaconda environment type &amp;ldquo;labelme&amp;rdquo; and the software should open up like below.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(pothole) C:\Users&amp;gt;labelme
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Open you image from your directory and click on Create Polygon and start labeling your images. Labelme saves your labels as json files with the same name as the image name. Place the json in the same directory as your images. An example of Labelme(right) along with Pixel Annotation Tool(left) is shown below. For this project I have labeled 400 images.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gallery/Labeling.jpg&#34; alt=&#34;Some of the Images in the dataset&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;model-training&#34;&gt;Model Training&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Create TFRecords:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After labeling our entire dataset we now have to generate TFRecords which serves as input for our model training. But before that we need to convert the json labelme labels into COCO format. 
I have taken the script provided by Gilber Tanner in his tutorial to perform this. You can also find this in my Github Repository labeled &amp;ldquo;labelme2coco.py&amp;rdquo;. Download this and place it onto the directory where your Train/ Test images are located. Now run the following commands:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;(pothole) C:\Users\models\research\object_detection\images&amp;gt;python labelme2coco.py train --output train.json
(pothole) C:\Users\models\research\object_detection\images&amp;gt;python labelme2coco.py test --output test.json
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now that the train/test data is in the COCO format we can now create the TFRecords using the create_coco_tf_record.py also created by Gilber Tanner. This script also needs to be placed and run from the object_detection folder.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-{r&#34; data-lang=&#34;{r&#34;&gt;python create_coco_tf_record.py --logtostderr --train_image_dir=images/train --test_image_dir=images/test --train_annotations_file=images/train.json --test_annotations_file=images/test.json --include_masks=True --output_dir=./
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You should find train.record and test.record in your object_detection folder.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Creating Label Map&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The label map links class names to ID numbers. Use a text editor like Sublime Text to create a &amp;ldquo;labelmap.pbtxt&amp;rdquo; and store it inside object_detection/training folder. Inside the folder write the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;  item {
  id: 1
  name: &#39;Pothole&#39;
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You can define as much as you want depending on the classes you want to detect but for the purpose of this project we are only interested in detecting potholes.&lt;/p&gt;
&lt;p&gt;This id should match with the id mentioned in your train.json and test.json files. Notice how it one number greater i.e here it is id: 0 but we mention id:1 in the labelmap file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;quot;categories&amp;quot;: [
    {
        &amp;quot;supercategory&amp;quot;: &amp;quot;Pothole&amp;quot;,
        &amp;quot;id&amp;quot;: 0,
        &amp;quot;name&amp;quot;: &amp;quot;Pothole&amp;quot;
    },
],
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Creating Training Configuration File:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now we need to create a training configuration file. From the &lt;a href=&#34;https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md&#34;&gt;tensorflow model zoo&lt;/a&gt; there are a variety of tensorflow models available for Mask RCNN but for the purpose of this project we are gonna use the &lt;a href=&#34;http://download.tensorflow.org/models/object_detection/mask_rcnn_inception_v2_coco_2018_01_28.tar.gz&#34;&gt;mask_rcnn_inception_v2_coco&lt;/a&gt; because of it&amp;rsquo;s speed. Download this and place it onto the object_detection folder. You can find the  mask_rcnn_inception_v2_coco.config file inside the samples/config folder. Copy this folder and place it into object_detection/training folder. Now we have to make the following changes to this config file:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Line 10: Change num_classes to the number of different objects you want the classifier to detect.(1 in this project&amp;rsquo;s case)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Line 126: Change fine_tune_checkpoint to:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;fine_tune_checkpoint: &amp;quot;&amp;lt;path&amp;gt;/models/research/object_detection/mask_rcnn_inception_v2_coco_2018_01_28/model.ckpt&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Line 142: Change input_path to the path of the train.records file:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;input_path: &amp;quot;&amp;lt;path&amp;gt;/models/research/object_detection/train.record&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Line 158: Change input_path to the path of the test.records file:
`&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;input_path: &amp;quot;&amp;lt;path&amp;gt;/models/research/object_detection/test.record&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Line 144 and 160: Change label_map_path to the path of the label map:&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;label_map_path: &amp;quot;&amp;lt;path&amp;gt;/models/research/object_detection/training/labelmap.pbtxt&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;Line 150: change num_example to the number of images in your test folder.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Training the Model:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Run the following command to start the training of the model from the object_detection folder:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python legacy/train.py --train_dir=training --pipeline_config_path=training/mask_rcnn_inception_v2_coco.config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After every interval the model saves the checkpoints in the training folder. It is a good idea to let it train till the loss is below 0.05. The time taken will depend on how powerful your GPU is.&lt;/p&gt;
&lt;p&gt;You can view the progress of your model by opening another Anaconda Prompt Window and changing the directory to the object_detection folder and typing the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(pothole) C:\models\research\object_detection&amp;gt;tensorboard --logdir=training
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This will create a webpage on your local machine YourPCName:6006, which can be viewed through a web browser. The TensorBoard page provides information and graphs that show how the training is progressing.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gallery/loss_graph.JPG&#34; alt=&#34;Tensorboard model training&#34;&gt;&lt;/p&gt;
&lt;p&gt;You can stop the training by pressing Ctrl+C while in the command prompt window. I recommend stopping after it has created the checkpoint in your folder this usually is done every 5-10 mins depending on your compute power. The checkpoint at the highest number of steps will be used to generate the frozen inference graph.&lt;/p&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Exporting Inference Graph&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Create a folder called &amp;ldquo;inference_graph&amp;rdquo; inside object_detection folder. Now we can create the frozen inference graph(.pb file) inside this folder. To do this issue the following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python export_inference_graph.py --input_type=image_tensor --pipeline_config_path=training/mask_rcnn_inception_v2_coco.config --trained_checkpoint_prefix=training/model.ckpt-2194 --output_directory=inference_graph
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This frozen inference graph is the object detection classifier.&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Testing the newly trained classifier&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To test the newly trained classifer you can make changes to the already existing object_detection.ipynb file or use the eval.ipynb file present in my Github Repo.&lt;/p&gt;
&lt;p&gt;Change the directory location for the labelmap, inference_graph, .config file and the test_images directory based on your location. You should get the follwing output:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gallery/Test.JPG&#34; alt=&#34;Testing Image&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;deploying-with-flask&#34;&gt;Deploying with Flask&lt;/h3&gt;
&lt;p&gt;Flask is a micro web framework written in Python developed by Armin Ronacher. We are going to use Flask to deploy our custom trained object detection model. You can find the beginner tutorial on their &lt;a href=&#34;https://flask.palletsprojects.com/en/1.1.x/quickstart/#a-minimal-application&#34;&gt;official documentation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We are going to be using the code present in the object_detection.ipynb file in our Flask app. The code is called &amp;ldquo;app.py&amp;rdquo; which is also present in my Github repository. In the beginning our app.py file we import our libraries and append our Python Path where the object detection api is located. Change this according to the location you have placed this file.&lt;/p&gt;
&lt;p&gt;The simple architecture of the Flask App can be described using the image below.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gallery/Flask_Architecture.png&#34; alt=&#34;Flask Architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;We take the image as input to the Custom Trained Mask RCNN model which based on the accuracy score then decides whether to give the coordinates or not. You can run the &amp;ldquo;app.py&amp;rdquo; by running the below command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;python app.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;gallery/Flaskapp.JPG&#34; alt=&#34;Flask App&#34;&gt;&lt;/p&gt;
&lt;p&gt;After running the above command we should get the below output. Copy this onto your browser for the web application to render the HTML pages. I have made a terrible job of this. You guys can create better interfaces or a better UI for this project by messing around with the HTML and CSS files. You can find all the output images below in the results section.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;This section just contains the various output images of the project.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is the first page after copying the URL from Anaconda Prompt onto your browser of your choice.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;gallery/FirstPage.JPG&#34; alt=&#34;First Page&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is the page after selecting and uploading an image of your choice. *&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;gallery/Upload.JPG&#34; alt=&#34;Upload Page&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This is the page after clicking on the submit button. Notice how the button below appears only when the score is above is 50%.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;gallery/ResultPage.JPG&#34; alt=&#34;Results Page&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;After clicking on the button below the output result that states to get the current position. I have zoomed out the map quite a bit to not reveal my location but you can get really precise and zoomed in coordinates. You can try to setup an architecture where you maintain a location database online so that the page can display those coordinates but for the purpose of this project we are just displaying the current location where the image was uploaded. So the image has to be taken and uploaded at the same spot.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;gallery/GoogleMaps.JPG&#34; alt=&#34;Google Maps Page&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Over 9300 deaths, 25000 injured in 3 years due to potholes - &lt;a href=&#34;https://www.indiatoday.in/india/story/over-9300-deaths-25000-injured-in-3-years-due-to-potholes-1294147-2018-07-24&#34;&gt;India Today&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Nienaber, S &amp;amp; Booysen, M.J. (Thinus) &amp;amp; Kroon, RS. (2015). &lt;a href=&#34;https://www.researchgate.net/publication/282807920_Dataset_of_images_used_for_pothole_detection?channel=doi&amp;amp;linkId=561ccc7b08ae78721fa2b350&amp;amp;showFulltext=true&#34;&gt;Dataset of images used for pothole detection. 10.13140/RG.2.1.3646.1520&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How To Train an Object Detection Classifier for Multiple Objects Using TensorFlow (GPU) on Windows 10 - &lt;a href=&#34;https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10&#34;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Custom Mask RCNN using Tensorflow Object Detection API - &lt;a href=&#34;https://medium.com/@vijendra1125/custom-mask-rcnn-using-tensorflow-object-detection-api-101149ce0765&#34;&gt;Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Train a Mask R-CNN model with the Tensorflow Object Detection API - &lt;a href=&#34;https://gilberttanner.com/blog/train-a-mask-r-cnn-model-with-the-tensorflow-object-detection-api&#34;&gt;Gilbert Tanner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;HTML Geolocation API - &lt;a href=&#34;https://www.w3schools.com/html/html5_geolocation.asp&#34;&gt;w3schools&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Fatigue Detection using Deep Learning</title>
      <link>/blogs/fatigue-detection/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/blogs/fatigue-detection/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;There has been a lot of research done on drowsiness detecton in vehicles but there hasn&amp;rsquo;t been any project that could detect fatigue levels through photos in a environment outside vehicles. In this I want to talk about this project based on the &lt;a href=&#34;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3738045/&#34;&gt;research&lt;/a&gt; to detect fatigue levels of a person through a photograph. For this project the main facial cues used to detect fatigue levels are: Undereyes, Eyes, Mouth, Nose and Skin.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;hypothesis.JPG&#34; alt=&#34;hypothesis&#34;&gt;&lt;/p&gt;
&lt;p&gt;The complexities of fatigue have drawn much attention from researchers across various disciplines. Short-term fatigue may cause safety issue while driving; thus, dynamic systems were designed to track driver fatigue. Longterm fatigue could lead to chronic syndromes, and eventually affect individuals physical and psychological health. Traditional methodologies of evaluating fatigue not only require sophisticated equipment but also consume enormous time.&lt;/p&gt;
&lt;h2 id=&#34;proposal&#34;&gt;Proposal&lt;/h2&gt;
&lt;p&gt;Therefore ,in this project the attempt was to develop a novel and efﬁcient method to predict individual’s fatigue rate by scrutinising human facial cues. The goal was to predict fatigue rate based on a single photo. This work represents a promising way to assess sleep-deprived fatigue, and the project will provide a viable and efﬁcient computational framework for user fatigue modelling in large-scale.&lt;/p&gt;
&lt;p&gt;Going over the architecture and the working of the system. In the picture below you can see the entire flow of the project or web application. 
&lt;img src=&#34;architecture.png&#34; alt=&#34;Architecture Image&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the beginning the input which is an image of a face is taken and fed into the system. The various facial cues that have been used for detecting the fatigue levels are then cropped out from the image and then fed into individual custom trained classification models which detect and output the fatigue levels. You can find the installation instructions and the steps to run the code in my &lt;a href=&#34;https://github.com/SamdenLepcha/Fatigue-Detection-using-Deep-Learning&#34;&gt;Github repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;That is in essence the high level gist of the project. But there were several technologies and individual processes that were done to bring the project together. These can be broadly divided into three phases. They are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Image Ingestion and Object Detection&lt;/li&gt;
&lt;li&gt;Classification&lt;/li&gt;
&lt;li&gt;Deployment&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;image-ingestion-and-object-detection-phase&#34;&gt;Image Ingestion and Object Detection Phase&lt;/h3&gt;
&lt;p&gt;This phase was performed using the famous computer vision library &lt;a href=&#34;https://opencv.org/&#34;&gt;&amp;ldquo;OpenCV&amp;rdquo;&lt;/a&gt;. There are many numerous applications of OpenCV. Some of them are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Face Recogniton&lt;/li&gt;
&lt;li&gt;Object Detection&lt;/li&gt;
&lt;li&gt;Street view image stitching&lt;/li&gt;
&lt;li&gt;Medical image analysis and many more&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;OpenCV&lt;/strong&gt; is a highly optimised library designed for real time applications. So what opencv does is that it converts the input image into numbers that can be understood using the &amp;ldquo;imread&amp;rdquo; function. So this image is fed and the necessary facial landmarks are cropped out. For detection of the face in general, eyes, nose , mouth we have used a pre-trained model readily available on the Internet.  This can be done using OpenCV , dlib and Python. You can read this &lt;a href=&#34;%22https://www.pyimagesearch.com/2017/04/10/detect-eyes-nose-lips-jaw-dlib-opencv-python/%22&#34;&gt;excellent tutorial&lt;/a&gt; from adrain pyimagesearch for facial extraction using dlib. But for undereyes there was not any readily available model therefore for this facial landmark a custom-trained object detection model was used.&lt;/p&gt;
&lt;p&gt;But to train the model data was required therefore,  the faces were downloaded from online sources such as unspalsh and google images and the undereyes were marked onto them. This was performed using &lt;a href=&#34;https://github.com/tzutalin/labelImg&#34;&gt;LabelImg&lt;/a&gt; which is an open source graphical image annotation tool.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;LabelImg.JPG&#34; alt=&#34;LabelImg&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then using the Tensorflow Object Detection API the new annotated data was used to train the model. The model used for training was inception-v2 with faster rcnn which are readily available for download in the tensorflow models repository.&lt;a href=&#34;%22http://download.tensorflow.org/models/object_detection/faster_rcnn_inception_v2_coco_2018_01_28.tar.gz%22&#34;&gt;You can download this model here&lt;/a&gt;. Inception v2 here is used as a feature extractor. I have used this &lt;a href=&#34;https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10&#34;&gt;tutorial&lt;/a&gt; by Evan for performing this process. This tutorial explains all the underlying steps required to train your very own object detection model.&lt;/p&gt;
&lt;p&gt;Some of the Images that were used for training the models are as follows:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;Images.JPG&#34; alt=&#34;Images&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;classification-phase&#34;&gt;Classification Phase&lt;/h3&gt;
&lt;p&gt;After these individual images were cropped out they were fed into classification models for training. These trained models would then be used later on for classifying new images. For the classification aspect of the facial cues &lt;strong&gt;EfficientNet&lt;/strong&gt; was used which is a very powerful architecture. Unlike traditional CNN architecture which scale network dimensions, such as width, depth and resolution EfficientNet uniformly scales each dimension with a fixed set of scaling coefficients. More details can be found in this &lt;a href=&#34;https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html&#34;&gt;article&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;efficientnet.JPG&#34; alt=&#34;EfficientNet&#34;&gt;&lt;/p&gt;
&lt;p&gt;An image from the blog which talks about the various scaling approaches.&lt;/p&gt;
&lt;h2 id=&#34;deployment-phase&#34;&gt;Deployment Phase&lt;/h2&gt;
&lt;p&gt;For the deployment of the entire project the &lt;strong&gt;&amp;ldquo;Flask API&amp;rdquo;&lt;/strong&gt; was used. It is an open-source micro web framework written in Python that is widely used for deployment purposes. For the front end or UI of the web application HTML/CSS was used. After each model derives the final results were calculated as a weighted sum of the facial cues.&lt;/p&gt;
&lt;p&gt;Some of the screenshots of the project can be found below:
&lt;img src=&#34;Upload.JPG&#34; alt=&#34;Uploading Image&#34;&gt;
&lt;img src=&#34;Result-D2.JPG&#34; alt=&#34;Result Image&#34;&gt;
&lt;img src=&#34;Result-D3.JPG&#34; alt=&#34;Result Extra Image&#34;&gt;
&lt;img src=&#34;Recon.JPG&#34; alt=&#34;Final Result Recommendation&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Journey Into Winning My First Data Science Hackathon</title>
      <link>/blogs/getting-started/</link>
      <pubDate>Mon, 28 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/blogs/getting-started/</guid>
      <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Note: This blog article only details the journey of taking part in a data science hackathon and does not contain the full technical details.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This was around the third week of January 2019 and just at the right time where our New Year Resolution energy were still pretty much present. My roommate and me had taken part in several hackathons before this and we had never reached close to the finishing line in any one of them. After completing several courses in data science it was frustrating to not see any results during these competitions. It was then my roommate had found a hackathon in a newly developed Indian based competition or hackathon website known as &lt;strong&gt;Machine Hack&lt;/strong&gt;. The title of this hackathon was &lt;strong&gt;&amp;ldquo;Republic Day Hackathon by TEG Analytics And AIM&amp;rdquo;&lt;/strong&gt;.
&lt;img src=&#34;gallery/hackathon1.JPG&#34; alt=&#34;Hackathon Picture in Machine Hack&#34;&gt;
It was a fresh start, a new data science hackathon where we would try our best again and hopefully get somewhere closer than our previous attempts. The Grand Prize stated that the entire team will get an International Trip for 4 days and 5 nights to anywhere in South East Asia. Wow! we could already see the different places that we could visit if we could win (Already dreaming about the trip at this point). The hackathon finale was scheduled for the 28th of January and we had one week to make something happen. There was a holiday right before the weekend and we intended to utilise every oppurtunity that we can get.&lt;/p&gt;
&lt;h2 id=&#34;continuing-on-&#34;&gt;Continuing on &amp;hellip;&lt;/h2&gt;
&lt;p&gt;Since the problem statement of the hackathon was to &lt;strong&gt;&amp;ldquo;Predicting Market Competitiveness for Medicare Insurance Products&amp;rdquo;&lt;/strong&gt; or in layman&amp;rsquo;s terms &amp;ldquo;Predict the enrollments of each insurance plan&amp;rdquo; we had to gain some domain knowledge since we did not have any prior knowledge on this field. Therefore for the first two days we decided to scour the internet and understand what &amp;ldquo;Medicare&amp;rdquo; is and how it operates. We needed to understand what the beneficiaries needed or preferred and how the Medicare Advantage companies structure their plans. Some really good resources were :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q1 Medicare &lt;a href=&#34;https://q1medicare.com/&#34;&gt;Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Kaiser Family Foundation &lt;a href=&#34;https://www.kff.org/&#34;&gt;Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Obivously Medicare&amp;rsquo;s Official &lt;a href=&#34;https://www.medicare.gov/&#34;&gt;Website&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;and Deft Research &lt;a href=&#34;https://www.deftresearch.com/&#34;&gt;Website&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After understanding some of the domain cost sharing terms such as &amp;ldquo;Deductible&amp;rdquo;, &amp;ldquo;Copayment&amp;rdquo;, &amp;ldquo;Co-insurance&amp;rdquo;, &amp;ldquo;MOOP (Maximum Out of Pocket Cost)&amp;rdquo; etc , we then decided to indulge in the datasets provided.&lt;/p&gt;
&lt;p&gt;The dataset provided consisted of three levels of information:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Plan Level Information&lt;/li&gt;
&lt;li&gt;Benefit Level Information&lt;/li&gt;
&lt;li&gt;Enrollment numbers at the State County Level&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We spent quite some time on the benefit level data but at that time due to college work (internal exams and assignments ughh) we could not decode this information and convert it into a form that we could have used. We instead spent our remaining time on the plan level information also known as the Landscape file which contains Demographics(State and County), Monthly Consolidated Premium (Medicare Part C + Part D), Annual Deductible, Maximum Out of Pocket Costs and Unique Plan Identifiers known as bid id (Contract Number + Plan ID + Segment Id). The Image below showcases our entire workflow or approach for this hackathon.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gallery/methodology_resized.JPG&#34; alt=&#34;Workflow Image&#34;&gt;&lt;/p&gt;
&lt;p&gt;After perfroming all the usual data science processes such as EDA, Preprocessing it was the modelling (P.S. If you want more technical information and methodology you can visit my &lt;a href=&#34;https://github.com/SamdenLepcha/Predict-Market-Competitiveness-For-Insurance-Products&#34;&gt;Github repo&lt;/a&gt;). It was obviously a regression problem where we were predicting a continuous variable i.e the number of enrollments. We first began with the thought process that we were gonna try a time series approach with GRU/LSTM models but that gave a pretty bad score in the leaderboard and with the lack of adequate time steps. We then switched over to tree based models and ensembling and as everyone knows from Kaggle XGboost is amazing!!!!. We used a stack approach of various models like Knn Regressor, Random Forest Regressor, GradientBoosting Regressor. AdaBoostRegressor and ExtraTrees Regressor. Intuition into the data compelled us to use stacked approach rather than plain ensembling of various models. In the end we ended up using a Stacked model and the implementation was performed using StackNet by kaz-Anova (I know once the highest Competitions Grandmaster in Kaggle). Check out his &lt;a href=&#34;https://github.com/kaz-Anova/StackNet&#34;&gt;amazing repository&lt;/a&gt; and we used the Python Implementation by H20.ai found &lt;a href=&#34;https://github.com/h2oai/pystacknet&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The new stacked model with the addition of the Lag variable that they provided on the last day of the hackathon enabled us to secure a Top 3 position on the Leaderboard.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gallery/Leaderboard.JPG&#34; alt=&#34;Leaderboard of the hackathon&#34;&gt;&lt;/p&gt;
&lt;p&gt;This meant that we had qualified for the final phase of the Hackathon which was gonna take place in Machine Learning Developers Summit 2019. We had cleared most of the hurdles but now it was time for the bigger stage. It was kind of nerve-racking to think now we had to present in front of hundreds of people. But nevertheless we put our best efforts into making a presentation for this event. It was a long cab ride to the event destination and all along the way we were practising our presenation lines. After making it to the event place and meeting the organising team and the competitors(1st and 2nd place teams) it was time for our presentation. We had lost some bit of confidence when we found out that the competitors were a team of professional data scientists while we were just a couple of data science enthusiasts at best. But we still decided to give it our all and then went on to present in front of the massive audience. We then sat for the competitors presentations as well and found out that they did not do anything that was extraordinary or in any way better than our approach. In fact their approach was very similar to ours as well.&lt;/p&gt;
&lt;p&gt;The results were to be announced at the final program of the event where they were awarding all the exceptional people in Data Science from various companies and universities. It was then with a little bit of suspense(cliché) that they announced the winners of the hackathon. You can already guess that the winners turned out be none other than my team &amp;ldquo;Neuron&amp;rdquo;. The image above the title is the team receiving the award from the CEO of TEG Analytics.&lt;/p&gt;
&lt;p&gt;You can find the code in my &lt;a href=&#34;https://github.com/SamdenLepcha/Predict-Market-Competitiveness-For-Insurance-Products&#34;&gt;Github&lt;/a&gt; repository, the video of our presentation on &lt;a href=&#34;https://www.youtube.com/watch?v=odrELjHvnsY&amp;amp;list=PL9Kc1zSa46OzbjNVPmRap4EUYx8tW2x5j&amp;amp;index=40&#34;&gt;Youtube&lt;/a&gt; and also an &lt;a href=&#34;https://analyticsindiamag.com/how-these-data-science-enthusiasts-from-christ-university-solved-our-insurance-products-hackathon/&#34;&gt;article&lt;/a&gt; published by &lt;strong&gt;Analytics India Magazine&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Ohh yeah not forgetting that the trip took me to Bali, Indonesia and yes it was AMAZINGGG!.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gallery/Bali.jpg&#34; alt=&#34;Bali Image&#34;&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
